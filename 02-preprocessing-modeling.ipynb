{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Preprocessing & Model Preparation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Performance Evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.23.2-cp38-cp38-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in e:\\springboard\\works\\support_ticket_label_classification\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\springboard\\works\\support_ticket_label_classification\\venv\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in e:\\springboard\\works\\support_ticket_label_classification\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Using legacy setup.py install for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "    Running setup.py install for sklearn: started\n",
      "    Running setup.py install for sklearn: finished with status 'done'\n",
      "Successfully installed scikit-learn-0.23.2 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'e:\\springboard\\works\\support_ticket_label_classification\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into pandas dataframe.\n",
    "file_location = '../../data/support_ticket.csv'\n",
    "\n",
    "df = pd.read_csv(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(579621, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'full_text', 'submitted_via', 'sub_label', 'label',\n",
       "       'word_count', 'unique_word_count', 'stop_word_count', 'url_count',\n",
       "       'mean_word_length', 'char_count', 'punctuation_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Tokenize a sentence into separate words, \n",
    "    and filter out words that are stopwords, punctuation, \n",
    "    numbers or have the form 'XXX' which indicates classified data.'''\n",
    "\n",
    "def tokenize_sentence(sentence: str, stop_words=True, punctuation=True, numbers=True, classified=True)->list:\n",
    "    \"\"\"\n",
    "    Tokenize a given string, and return the words as a list.\n",
    "    The function offers functionality to exclude the words that are either\n",
    "    1) a stopword 2) punctuation symbol 3) a number or 4) has the format 'XX'\n",
    "    or 'XXXX' indicates the words that classififed\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized = [x.lower() for x in word_tokenize(sentence)]\n",
    "    \n",
    "    if classified:\n",
    "        tokenized = [x for x in tokenized if x.lower() != 'xxxx' and\n",
    "                    x.lower() != 'xx' and x.lower() != 'xx/xx/xxxx']\n",
    "    \n",
    "    if stop_words:\n",
    "        tokenized = [x for x in tokenized if x not in stop_words]\n",
    "     \n",
    "    if punctuation:\n",
    "        tokenized = [x for x in tokenized if x not in string.punctuation]\n",
    "    \n",
    "    if numbers:\n",
    "        tokenized = [x for x in tokenized if not x.isdigit()]\n",
    "        \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence, return_form = 'string'):\n",
    "    \"\"\"\n",
    "    Lemmatize a given string . \n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        sentence: \n",
    "            Sentence that we want to lemmatize each word. The input can be\n",
    "            of the form of tokens (list) or the complete sentence (string).\n",
    "        return_form: \n",
    "            Format of the return function. Can be either a string\n",
    "            with the concatenated lemmatized words or a list of the \n",
    "            lemmatized words.\n",
    "    Returns:\n",
    "    -------\n",
    "        If join_string = True then the function returns the\n",
    "        lemmatized words as a sentence. Else it returns the words as a list.\n",
    "    \"\"\"\n",
    "    # Handle the case where the input is the string without being tokenized\n",
    "    if type(sentence) != list:\n",
    "        sentence = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if return_form == 'string':\n",
    "        return ' '.join([lemmatizer.lemmatize(word) for word in sentence])\n",
    "    else:\n",
    "        return [lemmatizer.lemmatize(word) for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the file so we do not need to reprocess each time\n",
    "pickle_processed_df_filename = 'complaints_processed.pkl'\n",
    "pickled_file_loc = os.path.join(project_dir, 'Data', pickle_processed_df_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to pickle\n",
    "complaints_processed.to_pickle(pickled_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickled file to df\n",
    "complaints_processed = pd.read_pickle(pickled_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "complaints_processed['Product_Id'] = label_encoder.fit_transform(complaints_processed['Product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also its good to have the categories as a dictionary\n",
    "product_map = complaints_processed.set_index('Product_Id').to_dict()['Product']\n",
    "product_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_processed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complaints_processed['Complaint_Clean']\n",
    "y = complaints_processed['Product_Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train.groupby(y_train).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))\n",
    "sns.barplot(x=sorted(y_train.unique()), y=y_train.groupby(y_train).count(), ax=ax1).set_title('Number of Complaints - Training Set')\n",
    "sns.barplot(x=sorted(y_test.unique()), y=y_test.groupby(y_test).count(), ax=ax2).set_title('Number of Complaints - Test Set')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
